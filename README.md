# Kedro LLM Evaluate

A Kedro plugin for evaluating and tracing Large Language Model (LLM) outputs in your data science pipelines.

## ğŸ¯ What is kedro_llm_evaluate?

`kedro_llm_evaluate` is a Kedro plugin that seamlessly integrates LLM evaluation and tracing capabilities into your Kedro pipelines. It provides automated tracking, evaluation metrics, and observability for LLM-powered data processing workflows, addressing the critical need for monitoring and improving LLM performance in production data pipelines.

**LLM evaluation frameworks comparison matrix:**

| Framework                 | Focus                                          | Pros                                                                                        | Cons                                                             | Community & OSS                                               | Best Fit in Kedro                                           |
| ------------------------- | ---------------------------------------------- | ------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- | ------------------------------------------------------------- | ----------------------------------------------------------- |
| **DeepEval**              | Python-native testing for LLMs                 | âœ… Pytest-like API<br>âœ… 14+ metrics incl. hallucination<br>âœ… Synthetic dataset generation    | âš ï¸ Early stage<br>âš ï¸ Less GUI or dashboard support               | ğŸŸ¡ Medium: Growing GitHub stars, active issues                | âœ… Easy Kedro integration as a node with prompt/result input |
| **MLFlow LLM Evaluate**   | LLM eval inside MLFlow ecosystem               | âœ… Familiar for MLFlow users<br>âœ… Auto logging + experiment tracking                         | âš ï¸ Not specialized for LLMs<br>âš ï¸ Limited metrics now            | ğŸŸ¢ Large MLFlow community<br>ğŸŸ¡ Limited LLM-specific traction | ğŸŸ¡ Best for orgs already using MLFlow with Kedro            |
| **RAGAs**                 | RAG pipeline quality (faithfulness, precision) | âœ… RAG-focused metrics<br>âœ… LangChain integration<br>âœ… Open source                           | âš ï¸ Narrow scope: RAG only<br>âš ï¸ Docs are minimal                 | ğŸŸ¡ Medium: Used in LangChain projects                         | âœ… Plug into Kedro RAG pipelines as post-processing node     |
| **Deepchecks**            | General ML + LLM validation (fairness, bias)   | âœ… Enterprise-grade ML testing<br>âœ… Bias/fairness modules                                    | âš ï¸ LLM support is recent<br>âš ï¸ May be heavy for simple LLM eval  | ğŸŸ¢ Active OSS, recognized brand                               | ğŸŸ¡ Best for teams also auditing traditional ML pipelines    |
| **Arize Phoenix**         | Open-source observability + eval               | âœ… LangChain + LlamaIndex support<br>âœ… Nice visual dashboards<br>âœ… Multi-model eval          | âš ï¸ High infra complexity<br>âš ï¸ Early adoption stage              | ğŸŸ¡ Medium community<br>Backed by Arize                        | ğŸŸ¡ Better fit with Kedro if observability is in scope       |
| **OpenAI Evals**          | Benchmark OpenAI models                        | âœ… Trusted by OpenAI<br>âœ… Integration with GPT evals<br>âœ… Flexible YAML format               | âš ï¸ Tied to OpenAI models<br>âš ï¸ Less plug-and-play                | ğŸŸ¡ Medium adoption, but not very active in open-source        | ğŸŸ¡ Use for evaluating OpenAI-specific tasks in Kedro        |
| **LM Evaluation Harness** | Academic benchmarks for LLMs                   | âœ… 60+ tasks<br>âœ… Supports HuggingFace, GPT<br>âœ… Gold standard for research                  | âš ï¸ Heavy setup<br>âš ï¸ Not easy to extend<br>âš ï¸ No RAG/prompt eval | ğŸŸ¢ Active research community (EleutherAI)                     | ğŸŸ¡ Best for model-to-model comparisons, not pipelines       |
| **PromptBench**           | Prompt engineering benchmarking                | âœ… Supports prompt variations<br>âœ… Adversarial testing<br>âœ… Supports multiple metrics        | âš ï¸ Limited maturity<br>âš ï¸ Small community                        | ğŸ”´ Low OSS adoption right now                                 | ğŸŸ¡ Useful in Kedro for testing prompt versions              |
| **LangFuse**              | Full LLM observability + eval + tracing        | âœ… Tracing, evals, prompt logs<br>âœ… Beautiful dashboard<br>âœ… Open source, LangChain friendly | âš ï¸ Requires infra setup<br>âš ï¸ Still maturing                     | ğŸŸ¡ Growing GitHub stars<br>âœ… DevRel team active               | âœ… Add as monitoring + evaluation layer to Kedro pipelines   |
| **Opik (Comet)**          | YAML-defined LLM eval pipeline                 | âœ… Clean YAML syntax<br>âœ… Flexible eval types<br>âœ… Open source<br>âœ… Comet integration        | âš ï¸ Lacks RAG-specific metrics<br>âš ï¸ Still maturing ecosystem     | ğŸŸ¡ Mid-sized user base, Comet-backed                          | âœ… Best plug-and-play choice for Kedro node eval step        |

## ğŸ—ï¸ Architecture

The plugin follows a modular architecture with the following components:

```
kedro_llm_evaluate/
â”œâ”€â”€ evaluators/          # Pluggable evaluation backends
â”‚   â”œâ”€â”€ base.py          # Base evaluator interface
â”‚   â””â”€â”€ opik/            # Opik evaluator implementation
â”‚       â”œâ”€â”€ opik_evaluator.py # Opik implementation
â”‚       â””â”€â”€ Dockerfile   # Docker setup for Opik
â”œâ”€â”€ tracing/             # LLM call tracing utilities
â”‚   â”œâ”€â”€ decorators.py    # @trace_llm_call decorator
â”‚   â”œâ”€â”€ context_manager.py # Context manager for tracing
â”‚   â””â”€â”€ logger.py        # Logging utilities
â”œâ”€â”€ utils/               # Core utilities
â”‚   â”œâ”€â”€ config_registry.py # Configuration management
â”‚   â””â”€â”€ factory.py       # Evaluator factory pattern
â”œâ”€â”€ hooks.py             # Kedro lifecycle hooks
â””â”€â”€ launchers/           # CLI extensions
    â””â”€â”€ cli.py          # CLI command implementations
```

### Key Components:

- **BaseLLMEvaluator**: Abstract interface for evaluation backends
- **OpikEvaluator**: Implementation using Opik for tracing and evaluation
- **Tracing System**: Decorators and context managers for automatic LLM call tracking
- **Configuration Registry**: Centralized configuration management
- **Kedro Hooks**: Automatic integration with Kedro pipeline lifecycle

## ğŸ“‹ Prerequisites

- Python 3.9+
- Kedro project
- Optional: Opik account for advanced evaluation features
- Optional: Docker if running Opik locally (https://www.comet.com/docs/opik/self-host/local_deployment)

## ğŸš€ Installation

```bash
pip install kedro-llm-evaluate
```

For Opik integration:
```bash
pip install "kedro-llm-evaluate[opik]"
```

## ğŸ“– Usage

### 1. Configuration

Add LLM evaluation configuration to your `parameters.yml`:

```yaml
# conf/base/parameters.yml
llm_evaluation:
  enabled: true
  evaluator: "opik"
  config:
    project_name: "my-llm-project"
    # Additional Opik configuration
```

### 2. Using the Decorator

Wrap your LLM functions with the tracing decorator:

```python
from kedro_llm_evaluate.tracing.decorators import trace_llm_call

@trace_llm_call(model="gpt-4", prompt_arg="prompt")
def summarize_text(prompt: str) -> str:
    # Your LLM call logic here
    response = llm_client.generate(prompt)
    return response
```

### 3. Using the Context Manager

For more control over tracing:

```python
from kedro_llm_evaluate.tracing.context_manager import trace_llm

def process_document(text: str) -> str:
    with trace_llm("gpt-4", "Summarize this text", {"document_id": "doc_123"}) as span:
        result = llm_client.generate(f"Summarize: {text}")
        if span:
            span.output_value = result
        return result
```

## ğŸ‰ Benefits & Value

### Immediate Benefits:
- **Automated LLM Tracking**: Zero-code instrumentation of LLM calls in your pipelines
- **Performance Monitoring**: Track response times, token usage, and success rates

### Business Value:
- **Reliability**: Ensure consistent LLM performance in production
- **Compliance**: Maintain audit trails for LLM decisions
- **Optimization**: Data-driven insights for prompt and model improvements
- **Scalability**: Monitor LLM performance across large-scale data pipelines

## ğŸ”® Next Steps After MVP

### Phase 1: Enhanced Evaluation
- [ ] Support for additional evaluation backends (DeepEval)
- [ ] Custom evaluation metrics framework
- [ ] Cost analysis and optimization recommendations
- [ ] Model drift detection

The plugin is particularly valuable for:
- **Data Science Teams** running LLM-powered pipelines in production
- **MLOps Engineers** needing observability for LLM components
- **Organizations** requiring audit trails and quality assurance for AI decisions
- **Research Teams** conducting systematic LLM evaluations at scale

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the [Apache 2.0](https://github.com/ravi-kumar-pilla/kedro-llm-evaluate/blob/main/LICENSE) License.

## ğŸ”— Related Links

- [Kedro Documentation](https://docs.kedro.org)
- [Opik Documentation](https://www.comet.com/docs/opik/)
- [Issue Tracker](https://github.com/kedro-org/kedro/issues)